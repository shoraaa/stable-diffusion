{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca5fb7e",
   "metadata": {},
   "source": [
    "# Stable Diffusion Demo \n",
    "\n",
    "This notebook demonstrates the Stable Diffusion pipeline with visualization of latent representations at each denoising step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d17019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "ALLOW_CUDA = True\n",
    "ALLOW_MPS = False\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
    "    DEVICE = \"mps\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e167374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load models and tokenizer\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5bd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A dog wearing a red scarf, sitting in a dreamy flower field at golden hour, highly detailed, realistic style\n",
      "Steps: 18\n",
      "Seed: 42\n",
      "CFG Scale: 8\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "prompt = \"A dog wearing a red scarf, sitting in a dreamy flower field at golden hour, highly detailed, realistic style\"\n",
    "uncond_prompt = \"do not change the dog's face, pose\"\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "# Image to image (optional)\n",
    "input_image = None\n",
    "# Uncomment to enable image to image\n",
    "# image_path = \"../images/dog.jpg\"\n",
    "# input_image = Image.open(image_path).convert(\"RGB\")\n",
    "# input_image.resize((512, 512))\n",
    "strength = 0.8\n",
    "\n",
    "# Sampler settings\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 18  # Reduced for faster execution and more frequent visualization\n",
    "seed = 42\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Steps: {num_inference_steps}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"CFG Scale: {cfg_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebef448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://c1786ff5e0549e5674.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "* Running on public URL: https://c1786ff5e0549e5674.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c1786ff5e0549e5674.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:06<00:00,  2.76it/s]\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:06<00:00,  2.69it/s]\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:06<00:00,  2.81it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import pipeline\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Global variable to control cancellation\n",
    "cancel_flag = threading.Event()\n",
    "\n",
    "# Global variable to store intermediate results\n",
    "intermediate_results = []\n",
    "\n",
    "def generate_image_realtime(prompt, input_image=None, strength=0.8, cfg_scale=8, num_inference_steps=18, seed=42, sampler=\"ddpm\", progress=gr.Progress()):\n",
    "    \"\"\"Generate images with true real-time updates showing each step of the process\"\"\"\n",
    "    global intermediate_results\n",
    "    \n",
    "    # Reset variables\n",
    "    cancel_flag.clear()\n",
    "    intermediate_results = []\n",
    "    \n",
    "    # Process input image if provided\n",
    "    if input_image:\n",
    "        input_image = input_image.convert(\"RGB\").resize((512, 512))\n",
    "    else:\n",
    "        input_image = None\n",
    "    \n",
    "    # Set deterministic seed\n",
    "    if seed is None or seed < 0:\n",
    "        seed = int(torch.randint(0, 2**32 - 1, (1,)).item())\n",
    "        print(f\"Using random seed: {seed}\")\n",
    "    \n",
    "    # Create a progress callback that stores each intermediate image\n",
    "    def store_progress(step, image_array):\n",
    "        \"\"\"Store each intermediate image for display\"\"\"\n",
    "        # Debug: Check image array values\n",
    "        print(f\"Step {step}: min={image_array.min():.4f}, max={image_array.max():.4f}, shape={image_array.shape}\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress((step) / num_inference_steps, desc=f\"Step {step}/{num_inference_steps}\")\n",
    "        \n",
    "        # Store the image - ensure values are in proper range for PIL\n",
    "        image_array_clipped = np.clip(image_array, 0, 255).astype(np.uint8)\n",
    "        intermediate_results.append(Image.fromarray(image_array_clipped))\n",
    "    \n",
    "    # Create placeholder image for initial display\n",
    "    placeholder = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "    placeholder_img = Image.fromarray(placeholder)\n",
    "    yield placeholder_img\n",
    "    \n",
    "    # Run the generation with our callback\n",
    "    output_image = pipeline.generate(\n",
    "        prompt=prompt,\n",
    "        uncond_prompt=\"\",\n",
    "        input_image=input_image,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=sampler,  # Use the selected sampler\n",
    "        n_inference_steps=num_inference_steps,\n",
    "        seed=seed,\n",
    "        models=models,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\",\n",
    "        tokenizer=tokenizer,\n",
    "        cancel_flag=cancel_flag,\n",
    "        progress_callback=store_progress\n",
    "    )\n",
    "    \n",
    "    # Now yield each intermediate result with a slight delay\n",
    "    # This creates a smooth visualization of the denoising process\n",
    "    for i, img in enumerate(intermediate_results):\n",
    "        progress((i + 1) / num_inference_steps, desc=f\"Visualizing step {i+1}/{num_inference_steps}\")\n",
    "        yield img\n",
    "    \n",
    "    # Finally show the complete image\n",
    "    if output_image is not None:\n",
    "        progress(1.0, desc=\"Generation complete!\")\n",
    "        # Debug: Check final output values\n",
    "        print(f\"Final output: min={output_image.min():.4f}, max={output_image.max():.4f}, shape={output_image.shape}\")\n",
    "        \n",
    "        # Make sure the final image is in proper range for PIL\n",
    "        output_image_clipped = np.clip(output_image, 0, 255).astype(np.uint8)\n",
    "        yield Image.fromarray(output_image_clipped)\n",
    "    else:\n",
    "        # If cancelled, show the last intermediate result\n",
    "        if intermediate_results:\n",
    "            yield intermediate_results[-1]\n",
    "        else:\n",
    "            # No results, show black image\n",
    "            yield placeholder_img\n",
    "\n",
    "def cancel_generation():\n",
    "    \"\"\"Cancel the generation process\"\"\"\n",
    "    cancel_flag.set()\n",
    "    return None\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(title=\"Stable Diffusion Real-Time Preview\") as demo:\n",
    "    gr.Markdown(\"# ðŸŽ¨ Stable Diffusion with Real-Time Visualization\")\n",
    "    gr.Markdown(\"Watch the image evolve step-by-step as the denoising process runs!\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            prompt_input = gr.Textbox(\n",
    "                label=\"Prompt\", \n",
    "                placeholder=\"Enter your prompt here...\",\n",
    "                value=\"A dog wearing a red scarf, sitting in a dreamy flower field at golden hour, highly detailed, realistic style\",\n",
    "                lines=3\n",
    "            )\n",
    "            image_input = gr.Image(label=\"Input Image (optional)\", type=\"pil\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                strength_slider = gr.Slider(0.1, 1.0, value=0.8, step=0.1, label=\"Strength\")\n",
    "                cfg_scale_slider = gr.Slider(1, 14, value=8, step=1, label=\"CFG Scale\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                steps_slider = gr.Slider(5, 50, value=18, step=1, label=\"Number of Steps\")\n",
    "                seed_input = gr.Number(value=42, label=\"Seed\")\n",
    "            \n",
    "            # Add sampler selection\n",
    "            sampler_dropdown = gr.Dropdown(\n",
    "                choices=[\"ddpm\", \"ddim\"], \n",
    "                value=\"ddpm\", \n",
    "                label=\"Sampler\",\n",
    "                info=\"DDPM is higher quality but slower. DDIM is faster but may have slight quality differences.\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                generate_button = gr.Button(\"ðŸŽ¨ Generate\", variant=\"primary\")\n",
    "                cancel_button = gr.Button(\"âŒ Cancel\", variant=\"secondary\")\n",
    "            \n",
    "            # Debug mode checkbox\n",
    "            debug_mode = gr.Checkbox(value=False, label=\"Enable Debug Mode\", info=\"Print debug info to console\")\n",
    "            \n",
    "        with gr.Column(scale=1):\n",
    "            # This is the image component that will be updated in-place\n",
    "            live_preview = gr.Image(\n",
    "                label=\"Live Generation Preview\",\n",
    "                height=512,\n",
    "                width=512\n",
    "            )\n",
    "    \n",
    "            # Add helpful instructions\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### ðŸŽ® How to Use\n",
    "            1. Enter a detailed prompt\n",
    "            2. Set your generation parameters\n",
    "            3. Select a sampler:\n",
    "               - **DDPM**: Original sampler, high quality but slower\n",
    "               - **DDIM**: Faster sampler, fewer steps needed for similar quality\n",
    "            4. Click \"Generate\" and watch the image evolve step by step!\n",
    "            \"\"\")\n",
    "    \n",
    "    # Connect the buttons to functions\n",
    "    generate_button.click(\n",
    "        generate_image_realtime,\n",
    "        inputs=[prompt_input, image_input, strength_slider, cfg_scale_slider, steps_slider, seed_input, sampler_dropdown],\n",
    "        outputs=live_preview\n",
    "    )\n",
    "    \n",
    "    cancel_button.click(\n",
    "        cancel_generation,\n",
    "        outputs=live_preview\n",
    "    )\n",
    "\n",
    "    # Add example prompts\n",
    "    gr.Markdown(\"### ðŸŒŸ Example Prompts\")\n",
    "    example_prompts = [\n",
    "        \"A majestic lion in a savanna at sunset, photorealistic, 4k\",\n",
    "        \"A futuristic city with flying cars, cyberpunk style, neon lights\",\n",
    "        \"A peaceful Japanese garden with cherry blossoms, watercolor painting style\",\n",
    "        \"A dragon flying over mountains, fantasy art, detailed scales\",\n",
    "        \"A vintage bicycle in a flower field, soft lighting, film photography\"\n",
    "    ]\n",
    "    \n",
    "    gr.Examples(\n",
    "        examples=[[prompt] for prompt in example_prompts],\n",
    "        inputs=[prompt_input],\n",
    "    )\n",
    "\n",
    "    # Add sampler comparison\n",
    "    gr.Markdown(\"\"\"\n",
    "    ### ðŸ“Š Sampler Comparison\n",
    "    \n",
    "    #### DDPM (Denoising Diffusion Probabilistic Models)\n",
    "    - The original sampler used in most diffusion models\n",
    "    - More accurate and higher quality results\n",
    "    - Slower as it requires more steps (25-50 recommended)\n",
    "    - Better for final high-quality images\n",
    "    \n",
    "    #### DDIM (Denoising Diffusion Implicit Models)\n",
    "    - Accelerated deterministic sampler\n",
    "    - Faster generation with fewer steps (10-25 recommended)\n",
    "    - Good quality with fewer steps\n",
    "    - Better when generation speed is important\n",
    "    - Deterministic results (same seed always gives same output)\n",
    "    \n",
    "    Try both samplers with different step counts to find the best balance of quality and speed!\n",
    "    \"\"\")\n",
    "\n",
    "demo.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different samplers side by side\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def compare_samplers(prompt, num_inference_steps=20, seed=42):\n",
    "    \"\"\"\n",
    "    Generate and compare images using different samplers with the same prompt and seed\n",
    "    \"\"\"\n",
    "    print(f\"Comparing samplers for prompt: '{prompt}' with {num_inference_steps} steps and seed {seed}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Collect intermediate images for comparison\n",
    "    ddpm_intermediates = []\n",
    "    ddim_intermediates = []\n",
    "    \n",
    "    # Define callbacks to collect intermediate steps\n",
    "    def ddpm_callback(step, image):\n",
    "        print(f\"DDPM Step {step}: min={image.min():.2f}, max={image.max():.2f}\")\n",
    "        ddpm_intermediates.append((step, image.copy()))\n",
    "        \n",
    "    def ddim_callback(step, image):\n",
    "        print(f\"DDIM Step {step}: min={image.min():.2f}, max={image.max():.2f}\")\n",
    "        ddim_intermediates.append((step, image.copy()))\n",
    "    \n",
    "    # Generate with DDPM\n",
    "    print(\"\\n[1/2] Generating with DDPM sampler...\")\n",
    "    ddpm_start = time.time()\n",
    "    try:\n",
    "        ddpm_image = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            uncond_prompt=\"\",\n",
    "            input_image=None,\n",
    "            strength=0.8,\n",
    "            do_cfg=True,\n",
    "            cfg_scale=7.5,\n",
    "            sampler_name=\"ddpm\",\n",
    "            n_inference_steps=num_inference_steps,\n",
    "            seed=seed,\n",
    "            models=models,\n",
    "            device=DEVICE,\n",
    "            idle_device=\"cpu\",\n",
    "            tokenizer=tokenizer,\n",
    "            progress_callback=ddpm_callback,\n",
    "        )\n",
    "        ddpm_time = time.time() - ddpm_start\n",
    "        print(f\"âœ“ DDPM completed in {ddpm_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DDPM generation failed: {e}\")\n",
    "        ddpm_image = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "        ddpm_time = time.time() - ddpm_start\n",
    "    \n",
    "    # Generate with DDIM\n",
    "    print(\"\\n[2/2] Generating with DDIM sampler...\")\n",
    "    ddim_start = time.time()\n",
    "    try:\n",
    "        ddim_image = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            uncond_prompt=\"\",\n",
    "            input_image=None,\n",
    "            strength=0.8,\n",
    "            do_cfg=True,\n",
    "            cfg_scale=7.5,\n",
    "            sampler_name=\"ddim\",\n",
    "            n_inference_steps=num_inference_steps,\n",
    "            seed=seed,\n",
    "            models=models,\n",
    "            device=DEVICE,\n",
    "            idle_device=\"cpu\",\n",
    "            tokenizer=tokenizer,\n",
    "            progress_callback=ddim_callback,\n",
    "        )\n",
    "        ddim_time = time.time() - ddim_start\n",
    "        print(f\"âœ“ DDIM completed in {ddim_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DDIM generation failed: {e}\")\n",
    "        ddim_image = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "        ddim_time = time.time() - ddim_start\n",
    "    \n",
    "    # Calculate speedup\n",
    "    if ddpm_time > 0 and ddim_time > 0:\n",
    "        speedup = ddpm_time / ddim_time\n",
    "        print(f\"\\nDDIM is {speedup:.2f}x faster than DDPM with the same number of steps\")\n",
    "    \n",
    "    # Debug: Check final image values\n",
    "    if ddpm_image is not None:\n",
    "        print(f\"DDPM final image: min={ddpm_image.min():.2f}, max={ddpm_image.max():.2f}, shape={ddpm_image.shape}\")\n",
    "    if ddim_image is not None:\n",
    "        print(f\"DDIM final image: min={ddim_image.min():.2f}, max={ddim_image.max():.2f}, shape={ddim_image.shape}\")\n",
    "    \n",
    "    # Make sure images are properly normalized\n",
    "    if ddpm_image is not None:\n",
    "        ddpm_image_display = np.clip(ddpm_image, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        ddpm_image_display = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "        \n",
    "    if ddim_image is not None:\n",
    "        ddim_image_display = np.clip(ddim_image, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        ddim_image_display = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Display the results side by side\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(ddpm_image_display)\n",
    "    plt.title(f\"DDPM Sampler\\n{num_inference_steps} steps, {ddpm_time:.2f}s\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(ddim_image_display)\n",
    "    plt.title(f\"DDIM Sampler\\n{num_inference_steps} steps, {ddim_time:.2f}s\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show intermediate steps comparison\n",
    "    if ddpm_intermediates and ddim_intermediates:\n",
    "        # Show middle step\n",
    "        mid_step_ddpm = len(ddpm_intermediates) // 2\n",
    "        mid_step_ddim = len(ddim_intermediates) // 2\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        if mid_step_ddpm < len(ddpm_intermediates):\n",
    "            plt.imshow(np.clip(ddpm_intermediates[mid_step_ddpm][1], 0, 255).astype(np.uint8))\n",
    "            plt.title(f\"DDPM - Step {ddpm_intermediates[mid_step_ddpm][0]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        if mid_step_ddim < len(ddim_intermediates):\n",
    "            plt.imshow(np.clip(ddim_intermediates[mid_step_ddim][1], 0, 255).astype(np.uint8))\n",
    "            plt.title(f\"DDIM - Step {ddim_intermediates[mid_step_ddim][0]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Sampler Comparison\\nPrompt: '{prompt}' - Seed: {seed}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ddpm_image, ddim_image, ddpm_time, ddim_time\n",
    "\n",
    "# Uncomment to run the sampler comparison\n",
    "# compare_samplers(\"A magical castle on a floating island with waterfalls, fantasy art style\", num_inference_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio interface for sampler comparison\n",
    "def create_sampler_comparison_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Stable Diffusion Sampler Comparison\")\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                prompt = gr.Textbox(label=\"Prompt\", value=\"A magical castle on a floating island with waterfalls, fantasy art style\")\n",
    "                steps_slider = gr.Slider(minimum=5, maximum=50, value=20, step=1, label=\"Number of steps\")\n",
    "                seed_number = gr.Number(value=42, label=\"Seed\", precision=0)\n",
    "                compare_btn = gr.Button(\"Compare Samplers\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"## DDPM Sampler\")\n",
    "                ddpm_output = gr.Image(label=\"DDPM Output\")\n",
    "                ddpm_time = gr.Textbox(label=\"DDPM Generation Time\")\n",
    "            with gr.Column():\n",
    "                gr.Markdown(\"## DDIM Sampler\")\n",
    "                ddim_output = gr.Image(label=\"DDIM Output\")\n",
    "                ddim_time = gr.Textbox(label=\"DDIM Generation Time\")\n",
    "                \n",
    "        with gr.Row():\n",
    "            result_text = gr.Textbox(label=\"Comparison Results\")\n",
    "        \n",
    "        def run_comparison(prompt_text, num_steps, seed_val):\n",
    "            try:\n",
    "                ddpm_img, ddim_img, ddpm_time_val, ddim_time_val = compare_samplers(\n",
    "                    prompt=prompt_text, \n",
    "                    num_inference_steps=int(num_steps), \n",
    "                    seed=int(seed_val)\n",
    "                )\n",
    "                \n",
    "                speedup = ddpm_time_val / ddim_time_val if ddim_time_val > 0 else 0\n",
    "                result = f\"DDPM took {ddpm_time_val:.2f}s, DDIM took {ddim_time_val:.2f}s\\n\"\n",
    "                result += f\"DDIM is {speedup:.2f}x faster than DDPM with {num_steps} steps\\n\"\n",
    "                \n",
    "                # Verify image quality\n",
    "                ddpm_std = np.std(ddpm_img) if ddpm_img is not None else 0\n",
    "                ddim_std = np.std(ddim_img) if ddim_img is not None else 0\n",
    "                \n",
    "                result += f\"\\nImage statistics:\\n\"\n",
    "                result += f\"DDPM: min={np.min(ddpm_img):.2f}, max={np.max(ddpm_img):.2f}, std={ddpm_std:.2f}\\n\"\n",
    "                result += f\"DDIM: min={np.min(ddim_img):.2f}, max={np.max(ddim_img):.2f}, std={ddim_std:.2f}\"\n",
    "                \n",
    "                # Make sure images are properly formatted for display\n",
    "                ddpm_img_display = np.clip(ddpm_img, 0, 255).astype(np.uint8) if ddpm_img is not None else np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "                ddim_img_display = np.clip(ddim_img, 0, 255).astype(np.uint8) if ddim_img is not None else np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "                \n",
    "                return [\n",
    "                    ddpm_img_display, \n",
    "                    f\"{ddpm_time_val:.2f}s\",\n",
    "                    ddim_img_display, \n",
    "                    f\"{ddim_time_val:.2f}s\",\n",
    "                    result\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                return [\n",
    "                    None,\n",
    "                    \"Error\",\n",
    "                    None,\n",
    "                    \"Error\",\n",
    "                    f\"Error during generation: {str(e)}\"\n",
    "                ]\n",
    "        \n",
    "        compare_btn.click(\n",
    "            fn=run_comparison,\n",
    "            inputs=[prompt, steps_slider, seed_number],\n",
    "            outputs=[ddpm_output, ddpm_time, ddim_output, ddim_time, result_text]\n",
    "        )\n",
    "        \n",
    "    return demo\n",
    "\n",
    "# Create and launch the sampler comparison interface\n",
    "sampler_comparison_demo = create_sampler_comparison_interface()\n",
    "# Uncomment to launch the interface\n",
    "# sampler_comparison_demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the sampler comparison interface\n",
    "sampler_comparison_demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed debugging of the DDIM sampler\n",
    "def debug_ddim_sampler(n_inference_steps=20, seed=42):\n",
    "    \"\"\"\n",
    "    Function to debug the DDIM sampler by visualizing intermediate latents and checking their statistics\n",
    "    \"\"\"\n",
    "    print(f\"Debugging DDIM sampler with {n_inference_steps} steps and seed {seed}\")\n",
    "    \n",
    "    # Setup DDIM sampler with specified seed\n",
    "    generator = torch.Generator(device=DEVICE)\n",
    "    generator.manual_seed(seed)\n",
    "    \n",
    "    # Initialize with eta=0.0 for deterministic sampling\n",
    "    ddim_sampler = DDIMSampler(generator, eta=0.0)\n",
    "    ddim_sampler.set_inference_timesteps(n_inference_steps)\n",
    "    \n",
    "    # Setup DDPM sampler with same seed for comparison\n",
    "    generator2 = torch.Generator(device=DEVICE)\n",
    "    generator2.manual_seed(seed)\n",
    "    ddpm_sampler = DDPMSampler(generator2)\n",
    "    ddpm_sampler.set_inference_timesteps(n_inference_steps)\n",
    "    \n",
    "    # Initialize random latents\n",
    "    latents_shape = (1, 4, 64, 64)  # Standard SD latent shape\n",
    "    latents_ddim = torch.randn(latents_shape, generator=generator, device=DEVICE)\n",
    "    latents_ddpm = latents_ddim.clone()  # Use same initial noise\n",
    "    \n",
    "    # Print initial latent statistics\n",
    "    print(f\"Initial latents - min: {latents_ddim.min().item():.4f}, max: {latents_ddim.max().item():.4f}, mean: {latents_ddim.mean().item():.4f}, std: {latents_ddim.std().item():.4f}\")\n",
    "    \n",
    "    # Dummy model output function (just for testing)\n",
    "    dummy_model = lambda x: torch.zeros_like(x)\n",
    "    \n",
    "    # Visualize the starting noise\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(latents_ddim[0, 0].detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.title(\"Initial Noise (Channel 0)\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Test the samplers with a simple denoising loop using a dummy model\n",
    "    print(\"\\nTesting DDIM sampler with dummy model output...\")\n",
    "    for i, timestep in enumerate(tqdm(ddim_sampler.timesteps)):\n",
    "        model_output = dummy_model(latents_ddim)\n",
    "        latents_ddim = ddim_sampler.step(timestep, latents_ddim, model_output)\n",
    "        \n",
    "        # Print statistics every few steps\n",
    "        if i % (n_inference_steps // 5) == 0 or i == len(ddim_sampler.timesteps) - 1:\n",
    "            print(f\"DDIM Step {i+1}/{len(ddim_sampler.timesteps)} - min: {latents_ddim.min().item():.4f}, max: {latents_ddim.max().item():.4f}, mean: {latents_ddim.mean().item():.4f}, std: {latents_ddim.std().item():.4f}\")\n",
    "    \n",
    "    # Compare with DDPM\n",
    "    print(\"\\nTesting DDPM sampler with dummy model output...\")\n",
    "    for i, timestep in enumerate(tqdm(ddpm_sampler.timesteps)):\n",
    "        model_output = dummy_model(latents_ddpm)\n",
    "        latents_ddpm = ddpm_sampler.step(timestep, latents_ddpm, model_output)\n",
    "        \n",
    "        # Print statistics every few steps\n",
    "        if i % (n_inference_steps // 5) == 0 or i == len(ddpm_sampler.timesteps) - 1:\n",
    "            print(f\"DDPM Step {i+1}/{len(ddpm_sampler.timesteps)} - min: {latents_ddpm.min().item():.4f}, max: {latents_ddpm.max().item():.4f}, mean: {latents_ddpm.mean().item():.4f}, std: {latents_ddpm.std().item():.4f}\")\n",
    "    \n",
    "    # Visualize the final latents (channel 0) for both samplers\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(latents_ddim[0, 0].detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.title(\"Final DDIM Latents (Channel 0)\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(latents_ddpm[0, 0].detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.title(\"Final DDPM Latents (Channel 0)\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return latents_ddim, latents_ddpm\n",
    "\n",
    "# Run the debug function\n",
    "# debug_latents_ddim, debug_latents_ddpm = debug_ddim_sampler(n_inference_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a side-by-side comparison of DDPM vs DDIM\n",
    "prompt = \"A beautiful mountain landscape with a lake and trees, digital art\"\n",
    "print(f\"Generating images with prompt: '{prompt}'\")\n",
    "\n",
    "# Run the comparison\n",
    "ddpm_image, ddim_image, ddpm_time, ddim_time = compare_samplers(\n",
    "    prompt=prompt, \n",
    "    num_inference_steps=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Show speedup\n",
    "if ddpm_time > 0 and ddim_time > 0:\n",
    "    print(f\"DDIM is {ddpm_time / ddim_time:.2f}x faster than DDPM with the same number of steps\")\n",
    "\n",
    "# Show image statistics for debugging\n",
    "def print_img_stats(name, img):\n",
    "    if img is not None:\n",
    "        print(f\"{name} image stats: min={img.min():.2f}, max={img.max():.2f}, mean={img.mean():.2f}, std={img.std():.2f}\")\n",
    "    else:\n",
    "        print(f\"{name} image is None\")\n",
    "\n",
    "print_img_stats(\"DDPM\", ddpm_image)\n",
    "print_img_stats(\"DDIM\", ddim_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9fa7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image-to-image generation with DDIM\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# First, let's add the missing set_strength method to DDIM\n",
    "def add_set_strength_to_ddim():\n",
    "    # Only add if not already present\n",
    "    if not hasattr(DDIMSampler, 'set_strength'):\n",
    "        code = \"\"\"\n",
    "    def set_strength(self, strength: float = 1.0) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        Set the denoising strength for image-to-image generation.\n",
    "        \n",
    "        This method is used in img2img pipelines where we start from an existing\n",
    "        image rather than pure noise. The strength parameter controls how much\n",
    "        of the original image structure is preserved.\n",
    "        \n",
    "        Args:\n",
    "            strength (float): Denoising strength between 0.0 and 1.0\n",
    "                            - 1.0: Start from pure noise (like txt2img)\n",
    "                            - 0.8: Add significant noise, major changes\n",
    "                            - 0.5: Moderate changes to original image\n",
    "                            - 0.2: Minor changes, preserve most structure\n",
    "                            - 0.0: No changes (return original image)\n",
    "        \\\"\\\"\\\"\n",
    "        # Calculate how many initial denoising steps to skip\n",
    "        # Higher strength = fewer skipped steps = more denoising = more changes\n",
    "        start_step = self.num_inference_steps - int(self.num_inference_steps * strength)\n",
    "        \n",
    "        # Skip the initial timesteps (start from partially noised image)\n",
    "        self.timesteps = self.timesteps[start_step:]\n",
    "        self.start_step = start_step\n",
    "        \n",
    "        print(f\"Set DDIM strength to {strength}, starting from step {start_step}/{self.num_inference_steps}\")\n",
    "        \"\"\"\n",
    "        # Add the method to the class\n",
    "        import types\n",
    "        DDIMSampler.set_strength = types.MethodType(\n",
    "            lambda self, strength=1.0: exec(code), \n",
    "            DDIMSampler\n",
    "        )\n",
    "        print(\"Added set_strength method to DDIMSampler\")\n",
    "\n",
    "# Add the method\n",
    "add_set_strength_to_ddim()\n",
    "\n",
    "# Now let's try image-to-image with DDIM\n",
    "def test_img2img():\n",
    "    # Create a simple gradient image as input\n",
    "    width, height = 512, 512\n",
    "    gradient = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # Create a simple gradient\n",
    "            gradient[y, x, 0] = int(255 * x / width)  # R increases from left to right\n",
    "            gradient[y, x, 1] = int(255 * y / height)  # G increases from top to bottom\n",
    "            gradient[y, x, 2] = 128  # Constant blue\n",
    "    \n",
    "    # Convert to PIL image\n",
    "    input_img = Image.fromarray(gradient)\n",
    "    \n",
    "    # Display the input image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(input_img)\n",
    "    plt.title(\"Input Image (Gradient)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Run image-to-image with both samplers\n",
    "    prompt = \"A beautiful sunset over mountains\"\n",
    "    strength = 0.75\n",
    "    print(f\"Running img2img with prompt: '{prompt}', strength: {strength}\")\n",
    "    \n",
    "    # Generate with DDPM\n",
    "    print(\"Generating with DDPM sampler...\")\n",
    "    ddpm_start = time.time()\n",
    "    try:\n",
    "        ddpm_image = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            uncond_prompt=\"\",\n",
    "            input_image=input_img,  # Pass the input image\n",
    "            strength=strength,      # Set the strength\n",
    "            do_cfg=True,\n",
    "            cfg_scale=7.5,\n",
    "            sampler_name=\"ddpm\",\n",
    "            n_inference_steps=20,\n",
    "            seed=42,\n",
    "            models=models,\n",
    "            device=DEVICE,\n",
    "            idle_device=\"cpu\",\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        ddpm_time = time.time() - ddpm_start\n",
    "        print(f\"âœ“ DDPM completed in {ddpm_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DDPM generation failed: {e}\")\n",
    "        ddpm_image = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "        ddpm_time = time.time() - ddpm_start\n",
    "    \n",
    "    # Generate with DDIM\n",
    "    print(\"Generating with DDIM sampler...\")\n",
    "    ddim_start = time.time()\n",
    "    try:\n",
    "        ddim_image = pipeline.generate(\n",
    "            prompt=prompt,\n",
    "            uncond_prompt=\"\",\n",
    "            input_image=input_img,  # Pass the input image\n",
    "            strength=strength,      # Set the strength\n",
    "            do_cfg=True,\n",
    "            cfg_scale=7.5,\n",
    "            sampler_name=\"ddim\",\n",
    "            n_inference_steps=20,\n",
    "            seed=42,\n",
    "            models=models,\n",
    "            device=DEVICE,\n",
    "            idle_device=\"cpu\",\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        ddim_time = time.time() - ddim_start\n",
    "        print(f\"âœ“ DDIM completed in {ddim_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DDIM generation failed: {e}\")\n",
    "        ddim_image = np.zeros((512, 512, 3), dtype=np.uint8)\n",
    "        ddim_time = time.time() - ddim_start\n",
    "    \n",
    "    # Display the results side by side\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(input_img)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(ddpm_image)\n",
    "    plt.title(f\"DDPM Result ({ddpm_time:.2f}s)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(ddim_image)\n",
    "    plt.title(f\"DDIM Result ({ddim_time:.2f}s)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ddpm_image, ddim_image\n",
    "\n",
    "# Uncomment to run the test\n",
    "# img2img_ddpm, img2img_ddim = test_img2img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a proper set_strength method to the DDIM sampler file\n",
    "set_strength_code = \"\"\"    def set_strength(self, strength: float = 1.0) -> None:\n",
    "        \\\"\\\"\\\"\n",
    "        Set the denoising strength for image-to-image generation.\n",
    "        \n",
    "        This method is used in img2img pipelines where we start from an existing\n",
    "        image rather than pure noise. The strength parameter controls how much\n",
    "        of the original image structure is preserved.\n",
    "        \n",
    "        Args:\n",
    "            strength (float): Denoising strength between 0.0 and 1.0\n",
    "                            - 1.0: Start from pure noise (like txt2img)\n",
    "                            - 0.8: Add significant noise, major changes\n",
    "                            - 0.5: Moderate changes to original image\n",
    "                            - 0.2: Minor changes, preserve most structure\n",
    "                            - 0.0: No changes (return original image)\n",
    "        \\\"\\\"\\\"\n",
    "        # Calculate how many initial denoising steps to skip\n",
    "        # Higher strength = fewer skipped steps = more denoising = more changes\n",
    "        start_step = self.num_inference_steps - int(self.num_inference_steps * strength)\n",
    "        \n",
    "        # Skip the initial timesteps (start from partially noised image)\n",
    "        self.timesteps = self.timesteps[start_step:]\n",
    "        self.start_step = start_step\n",
    "        \n",
    "        print(f\"Set DDIM strength to {strength}, starting from step {start_step}/{self.num_inference_steps}\")\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "ddim_path = os.path.join(\"/home/shora/Research/stable-diffusion/sd/ddim.py\")\n",
    "\n",
    "# Check if the method already exists in the file\n",
    "with open(ddim_path, 'r') as f:\n",
    "    ddim_content = f.read()\n",
    "\n",
    "if \"def set_strength\" not in ddim_content:\n",
    "    # Find the last method in the class\n",
    "    last_method_index = ddim_content.rfind(\"def \")\n",
    "    # Find the end of this method by locating the next line with less indentation\n",
    "    lines = ddim_content.splitlines()\n",
    "    method_line = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"def \" in line and \"def set_strength\" not in line:\n",
    "            method_line = i\n",
    "            \n",
    "    if method_line is not None:\n",
    "        # Find the end of this method\n",
    "        indentation = len(lines[method_line]) - len(lines[method_line].lstrip())\n",
    "        end_line = None\n",
    "        for i in range(method_line + 1, len(lines)):\n",
    "            if lines[i].strip() and len(lines[i]) - len(lines[i].lstrip()) <= indentation:\n",
    "                end_line = i\n",
    "                break\n",
    "        \n",
    "        if end_line is None:\n",
    "            end_line = len(lines)\n",
    "        \n",
    "        # Insert the new method\n",
    "        lines.insert(end_line, set_strength_code)\n",
    "        new_content = \"\\n\".join(lines)\n",
    "        \n",
    "        # Write back to the file\n",
    "        with open(ddim_path, 'w') as f:\n",
    "            f.write(new_content)\n",
    "        \n",
    "        print(\"Added set_strength method to DDIM sampler file\")\n",
    "    else:\n",
    "        print(\"Could not find a suitable location to add the method\")\n",
    "else:\n",
    "    print(\"set_strength method already exists in the DDIM sampler file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3bdd9",
   "metadata": {},
   "source": [
    "# DDPM vs DDIM Sampler Comparison\n",
    "\n",
    "This notebook demonstrates the implementation and comparison of two diffusion model samplers:\n",
    "\n",
    "## 1. DDPM (Denoising Diffusion Probabilistic Models)\n",
    "- **Nature**: Stochastic sampling process\n",
    "- **Algorithm**: Follows the original DDPM paper by Ho et al., 2020\n",
    "- **Behavior**: Adds random noise at each sampling step\n",
    "- **Performance**: \n",
    "  - Higher quality, especially for complex details\n",
    "  - More coherent outputs\n",
    "  - Slower generation (more compute-intensive)\n",
    "\n",
    "## 2. DDIM (Denoising Diffusion Implicit Models)\n",
    "- **Nature**: Deterministic sampling process (when Î·=0)\n",
    "- **Algorithm**: Follows the DDIM paper by Song et al., 2020\n",
    "- **Behavior**: Takes larger steps with deterministic updates\n",
    "- **Performance**:\n",
    "  - Faster generation (typically 2-4x faster than DDPM)\n",
    "  - Requires fewer inference steps for similar quality\n",
    "  - Can be made stochastic by adjusting Î· parameter (0=deterministic, 1=DDPM-like)\n",
    "  - Allows for controlled interpolation in latent space\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "Both samplers follow the same general approach:\n",
    "1. Start with random noise\n",
    "2. Iteratively apply denoising steps to reach clean image\n",
    "3. Use U-Net to predict noise at each step\n",
    "\n",
    "The key differences are in how the sampling steps are calculated:\n",
    "- DDPM uses the full Markovian stochastic process\n",
    "- DDIM uses a non-Markovian deterministic process that can skip steps\n",
    "\n",
    "## Debug Notes\n",
    "\n",
    "If you encounter issues with the DDIM sampler:\n",
    "1. Check for numerical stability issues in the calculation\n",
    "2. Ensure proper tensor device handling\n",
    "3. Add small epsilon values to prevent division by zero or sqrt of negative values\n",
    "4. Monitor for NaN/Inf values in the generation process\n",
    "\n",
    "You can use the debug tools in this notebook to compare outputs and diagnose potential issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-stable-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
