{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca5fb7e",
   "metadata": {},
   "source": [
    "# Stable Diffusion Demo \n",
    "\n",
    "This notebook demonstrates the Stable Diffusion pipeline with visualization of latent representations at each denoising step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d17019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "ALLOW_CUDA = True\n",
    "ALLOW_MPS = False\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
    "    DEVICE = \"mps\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e167374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load models and tokenizer\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5bd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "prompt = \"A dog wearing a red scarf, sitting in a dreamy flower field at golden hour, highly detailed, realistic style\"\n",
    "negative_prompt = \"blurry, low quality, distorted, bad anatomy, extra limbs\"  # What to avoid\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "# Image to image (optional)\n",
    "input_image = None\n",
    "# Uncomment to enable image to image\n",
    "image_path = \"../images/dog.jpg\"\n",
    "# input_image = Image.open(image_path).convert(\"RGB\")\n",
    "# input_image.resize((512, 512))\n",
    "strength = 0.8\n",
    "\n",
    "# Sampler settings - Available options: \"ddim\", \"ddpm\", \"euler\"\n",
    "sampler = \"ddim\"  # Default changed to DDIM for faster generation\n",
    "num_inference_steps = 20  # Reduced for faster execution with DDIM/Euler (DDPM may need 50+)\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebef448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://2d6e7231fda5cd4c59.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2d6e7231fda5cd4c59.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2240, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1747, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_454803/4075034871.py\", line 71, in generate_inpaint\n",
      "    result = inpainting.inpaint(\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/sd/inpainting.py\", line 213, in inpaint\n",
      "    masked_image_tensor = prepare_masked_image(image, mask_tensor).to(device)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shora/Research/stable-diffusion/sd/inpainting.py\", line 125, in prepare_masked_image\n",
      "    image_tensor = image_tensor * (1 - mask_expanded) + 0.0 * mask_expanded\n",
      "                   ~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import inpainting\n",
    "import random\n",
    "import numpy as np\n",
    "from threading import Event\n",
    "\n",
    "cancel_flag = Event()\n",
    "\n",
    "def generate_txt2img(prompt, negative_prompt, strength, cfg_scale, num_inference_steps, seed, sampler_name):\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "    output_image = pipeline.generate(\n",
    "        prompt=prompt,\n",
    "        uncond_prompt=negative_prompt,\n",
    "        input_image=None,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=sampler_name,\n",
    "        n_inference_steps=num_inference_steps,\n",
    "        seed=seed,\n",
    "        models=models,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\",\n",
    "        tokenizer=tokenizer,\n",
    "        cancel_flag=cancel_flag,\n",
    "    )\n",
    "    if output_image is None:\n",
    "        return None\n",
    "    return Image.fromarray(output_image)\n",
    "\n",
    "def generate_img2img(prompt, negative_prompt, input_image, strength, cfg_scale, num_inference_steps, seed, sampler_name):\n",
    "    if input_image is None:\n",
    "        raise gr.Error(\"Please upload an input image\")\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "    output_image = pipeline.generate(\n",
    "        prompt=prompt,\n",
    "        uncond_prompt=negative_prompt,\n",
    "        input_image=input_image,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=sampler_name,\n",
    "        n_inference_steps=num_inference_steps,\n",
    "        seed=seed,\n",
    "        models=models,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\",\n",
    "        tokenizer=tokenizer,\n",
    "        cancel_flag=cancel_flag,\n",
    "    )\n",
    "    if output_image is None:\n",
    "        return None\n",
    "    return Image.fromarray(output_image)\n",
    "\n",
    "def generate_inpaint(image, mask, prompt, negative_prompt, strength, cfg_scale, num_steps, seed, sampler_name):\n",
    "    if image is None or mask is None:\n",
    "        raise gr.Error(\"Please upload both image and mask\")\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "    mask = mask.convert(\"L\").resize(image.size)\n",
    "    mask_np = np.array(mask)\n",
    "    original_size = image.size\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "    result = inpainting.inpaint(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        mask=mask_np,\n",
    "        uncond_prompt=negative_prompt,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=sampler_name,\n",
    "        n_inference_steps=num_steps,\n",
    "        models=models,\n",
    "        tokenizer=tokenizer,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\"\n",
    "    )\n",
    "    result_image = Image.fromarray(result)\n",
    "    result_image = result_image.resize(original_size, resample=Image.LANCZOS)\n",
    "    return result_image\n",
    "\n",
    "# Available samplers\n",
    "SAMPLERS = [\"ddim\", \"ddpm\", \"euler\"]\n",
    "\n",
    "with gr.Blocks(css=\".progress-bar, .svelte-1ipelgc {display: none !important;}\") as demo:\n",
    "    gr.Markdown(\"# Stable Diffusion All-in-One Demo\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Text-to-Image\"):\n",
    "            t2i_prompt = gr.Textbox(label=\"📝 Prompt\", placeholder=\"Enter your prompt here...\", lines=2)\n",
    "            t2i_negative = gr.Textbox(label=\"🚫 Negative Prompt\", placeholder=\"What to avoid in the image...\", value=\"blurry, low quality, distorted\", lines=1)\n",
    "            with gr.Row():\n",
    "                t2i_sampler = gr.Dropdown(choices=SAMPLERS, value=\"ddim\", label=\"Sampler\")\n",
    "                t2i_strength = gr.Slider(0.1, 1.0, value=0.8, step=0.1, label=\"Strength\")\n",
    "            with gr.Row():\n",
    "                t2i_cfg = gr.Slider(1, 14, value=8, step=1, label=\"CFG Scale\")\n",
    "                t2i_steps = gr.Slider(1, 50, value=20, step=1, label=\"Number of Inference Steps\")\n",
    "            t2i_seed = gr.Number(value=-1, label=\"Seed (-1 = random)\")\n",
    "            t2i_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "            t2i_output = gr.Image(label=\"Generated Image\")\n",
    "            t2i_btn.click(\n",
    "                generate_txt2img,\n",
    "                inputs=[t2i_prompt, t2i_negative, t2i_strength, t2i_cfg, t2i_steps, t2i_seed, t2i_sampler],\n",
    "                outputs=t2i_output,\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"Image-to-Image\"):\n",
    "            i2i_prompt = gr.Textbox(label=\"📝 Prompt\", placeholder=\"Enter your prompt here...\", lines=2)\n",
    "            i2i_negative = gr.Textbox(label=\"🚫 Negative Prompt\", placeholder=\"What to avoid in the image...\", value=\"blurry, low quality, distorted\", lines=1)\n",
    "            i2i_image = gr.Image(label=\"Input Image\", type=\"pil\")\n",
    "            with gr.Row():\n",
    "                i2i_sampler = gr.Dropdown(choices=SAMPLERS, value=\"ddim\", label=\"Sampler\")\n",
    "                i2i_strength = gr.Slider(0.1, 1.0, value=0.8, step=0.1, label=\"Strength\")\n",
    "            with gr.Row():\n",
    "                i2i_cfg = gr.Slider(1, 14, value=8, step=1, label=\"CFG Scale\")\n",
    "                i2i_steps = gr.Slider(1, 50, value=20, step=1, label=\"Number of Inference Steps\")\n",
    "            i2i_seed = gr.Number(value=-1, label=\"Seed (-1 = random)\")\n",
    "            i2i_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "            i2i_output = gr.Image(label=\"Generated Image\")\n",
    "            i2i_btn.click(\n",
    "                generate_img2img,\n",
    "                inputs=[i2i_prompt, i2i_negative, i2i_image, i2i_strength, i2i_cfg, i2i_steps, i2i_seed, i2i_sampler],\n",
    "                outputs=i2i_output,\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"Inpainting\"):\n",
    "            inp_image = gr.Image(label=\"Upload Your Image\", type=\"pil\")\n",
    "            inp_mask = gr.Image(label=\"Draw Mask (white = inpaint)\", type=\"pil\")\n",
    "            inp_prompt = gr.Textbox(label=\"📝 Prompt\", lines=2, placeholder=\"e.g. a mountain with a castle\")\n",
    "            inp_negative = gr.Textbox(label=\"🚫 Negative Prompt\", value=\"blurry, low quality\", lines=1)\n",
    "            with gr.Row():\n",
    "                inp_sampler = gr.Dropdown(choices=SAMPLERS, value=\"ddim\", label=\"Sampler\")\n",
    "                inp_strength = gr.Slider(0.1, 1.0, step=0.1, value=0.8, label=\"Strength\")\n",
    "            with gr.Row():\n",
    "                inp_cfg = gr.Slider(1.0, 20.0, step=0.5, value=7.5, label=\"CFG Scale\")\n",
    "                inp_steps = gr.Slider(10, 100, step=5, value=30, label=\"Denoising Steps\")\n",
    "            inp_seed = gr.Number(value=-1, precision=0, label=\"Seed (-1 = random)\")\n",
    "            inp_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "            inp_output = gr.Image(label=\"🖼️ Output Image\")\n",
    "            inp_btn.click(\n",
    "                generate_inpaint,\n",
    "                inputs=[inp_image, inp_mask, inp_prompt, inp_negative, inp_strength, inp_cfg, inp_steps, inp_seed, inp_sampler],\n",
    "                outputs=inp_output,\n",
    "            )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inpainting functionality with negative prompts\n",
    "print(\"🔧 Testing inpainting functionality with negative prompts...\")\n",
    "\n",
    "# Create a simple test image and mask\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Create a 512x512 test image (blue background)\n",
    "test_image = np.full((512, 512, 3), [100, 150, 255], dtype=np.uint8)\n",
    "test_image_pil = Image.fromarray(test_image)\n",
    "\n",
    "# Create a simple mask (white circle in center for inpainting)\n",
    "mask = np.zeros((512, 512), dtype=np.uint8)\n",
    "center = 256\n",
    "radius = 100\n",
    "y, x = np.ogrid[:512, :512]\n",
    "mask_circle = (x - center)**2 + (y - center)**2 <= radius**2\n",
    "mask[mask_circle] = 255\n",
    "mask_pil = Image.fromarray(mask)\n",
    "\n",
    "test_prompt = \"a beautiful red rose in the center\"\n",
    "test_negative = \"blurry, low quality, distorted, cartoon\"\n",
    "\n",
    "try:\n",
    "    print(\"🎯 Running inpainting test...\")\n",
    "    result = inpainting.inpaint(\n",
    "        prompt=test_prompt,\n",
    "        image=test_image_pil,\n",
    "        mask=mask,\n",
    "        uncond_prompt=test_negative,\n",
    "        strength=0.9,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=7.5,\n",
    "        sampler_name=\"ddim\",\n",
    "        n_inference_steps=15,\n",
    "        models=models,\n",
    "        tokenizer=tokenizer,\n",
    "        seed=42,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Save the result\n",
    "    result_image = Image.fromarray(result)\n",
    "    result_image.save(\"../data/outputs/inpainting_test_with_negative.png\")\n",
    "    print(\"✅ Inpainting test successful!\")\n",
    "    print(\"   📁 Result saved: ../data/outputs/inpainting_test_with_negative.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Inpainting test failed: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-stable-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
