{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca5fb7e",
   "metadata": {},
   "source": [
    "# Stable Diffusion Demo \n",
    "\n",
    "This notebook demonstrates the Stable Diffusion pipeline with visualization of latent representations at each denoising step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d17019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_18480\\924915005.py:14: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n"
     ]
    }
   ],
   "source": [
    "import model_loader\n",
    "import pipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "ALLOW_CUDA = False\n",
    "ALLOW_MPS = False\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
    "    DEVICE = \"mps\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e167374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load models and tokenizer\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5bd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A dog wearing a red scarf, sitting in a dreamy flower field at golden hour, highly detailed, realistic style\n",
      "Steps: 18\n",
      "Seed: 42\n",
      "CFG Scale: 8\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "prompt = \"A dog wearing a red scarf, sitting in a dreamy flower field at golden hour, highly detailed, realistic style\"\n",
    "uncond_prompt = \"do not change the dog's face, pose\"\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "# Image to image (optional)\n",
    "input_image = None\n",
    "# Uncomment to enable image to image\n",
    "image_path = \"../images/dog.jpg\"\n",
    "input_image = Image.open(image_path).convert(\"RGB\")\n",
    "input_image.resize((512, 512))\n",
    "strength = 0.8\n",
    "\n",
    "# Sampler settings\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 18  # Reduced for faster execution and more frequent visualization\n",
    "seed = 42\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Steps: {num_inference_steps}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"CFG Scale: {cfg_scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1c152",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebef448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DDPM sampler with 5 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\ACER\\OneDrive - vnu.edu.vn\\Documents\\Term 4\\Cac van de\\prj\\stable-diffusion\\sd\\pipeline.py:416: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:36<00:00, 43.23s/it]\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import pipeline\n",
    "import inpainting\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Global variable to control cancellation\n",
    "cancel_flag = threading.Event()\n",
    "\n",
    "def generate_txt2img(prompt, strength, cfg_scale, num_inference_steps, seed):\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "    output_image = pipeline.generate(\n",
    "        prompt=prompt,\n",
    "        uncond_prompt=\"\",\n",
    "        input_image=None,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=num_inference_steps,\n",
    "        seed=seed,\n",
    "        models=models,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\",\n",
    "        tokenizer=tokenizer,\n",
    "        cancel_flag=cancel_flag,\n",
    "    )\n",
    "    if output_image is None:\n",
    "        return None\n",
    "    return Image.fromarray(output_image)\n",
    "\n",
    "def generate_img2img(prompt, input_image, strength, cfg_scale, num_inference_steps, seed):\n",
    "    if input_image is None:\n",
    "        raise gr.Error(\"Please upload an input image\")\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "    output_image = pipeline.generate(\n",
    "        prompt=prompt,\n",
    "        uncond_prompt=\"\",\n",
    "        input_image=input_image,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=num_inference_steps,\n",
    "        seed=seed,\n",
    "        models=models,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\",\n",
    "        tokenizer=tokenizer,\n",
    "        cancel_flag=cancel_flag,\n",
    "    )\n",
    "    if output_image is None:\n",
    "        return None\n",
    "    return Image.fromarray(output_image)\n",
    "\n",
    "def generate_inpaint(image, mask, prompt, negative_prompt, strength, cfg_scale, num_steps, seed):\n",
    "    if image is None or mask is None:\n",
    "        raise gr.Error(\"Please upload both image and mask\")\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "    mask = mask.convert(\"L\").resize(image.size)\n",
    "    mask_np = np.array(mask)\n",
    "    original_size = image.size\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "    result = inpainting.inpaint(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        mask=mask_np,\n",
    "        uncond_prompt=negative_prompt,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=num_steps,\n",
    "        models=models,\n",
    "        tokenizer=tokenizer,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\"\n",
    "    )\n",
    "    result_image = Image.fromarray(result)\n",
    "    result_image = result_image.resize(original_size, resample=Image.LANCZOS)\n",
    "    return result_image\n",
    "\n",
    "with gr.Blocks(css=\".progress-bar, .svelte-1ipelgc {display: none !important;}\") as demo:\n",
    "    gr.Markdown(\"# Stable Diffusion All-in-One Demo\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Text-to-Image\"):\n",
    "            t2i_prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Enter your prompt here...\")\n",
    "            t2i_strength = gr.Slider(0.1, 1.0, value=0.8, step=0.1, label=\"Strength\")\n",
    "            t2i_cfg = gr.Slider(1, 14, value=8, step=1, label=\"CFG Scale\")\n",
    "            t2i_steps = gr.Slider(1, 50, value=18, step=1, label=\"Number of Inference Steps\")\n",
    "            t2i_seed = gr.Number(value=-1, label=\"Seed (-1 = random)\")\n",
    "            t2i_btn = gr.Button(\"Generate\")\n",
    "            t2i_output = gr.Image(label=\"Generated Image\")\n",
    "            t2i_btn.click(\n",
    "                generate_txt2img,\n",
    "                inputs=[t2i_prompt, t2i_strength, t2i_cfg, t2i_steps, t2i_seed],\n",
    "                outputs=t2i_output,\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"Image-to-Image\"):\n",
    "            i2i_prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Enter your prompt here...\")\n",
    "            i2i_image = gr.Image(label=\"Input Image\", type=\"pil\")\n",
    "            i2i_strength = gr.Slider(0.1, 1.0, value=0.8, step=0.1, label=\"Strength\")\n",
    "            i2i_cfg = gr.Slider(1, 14, value=8, step=1, label=\"CFG Scale\")\n",
    "            i2i_steps = gr.Slider(1, 50, value=18, step=1, label=\"Number of Inference Steps\")\n",
    "            i2i_seed = gr.Number(value=-1, label=\"Seed (-1 = random)\")\n",
    "            i2i_btn = gr.Button(\"Generate\")\n",
    "            i2i_output = gr.Image(label=\"Generated Image\")\n",
    "            i2i_btn.click(\n",
    "                generate_img2img,\n",
    "                inputs=[i2i_prompt, i2i_image, i2i_strength, i2i_cfg, i2i_steps, i2i_seed],\n",
    "                outputs=i2i_output,\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"Inpainting\"):\n",
    "            inp_image = gr.Image(label=\"Upload Your Image\", type=\"pil\")\n",
    "            inp_mask = gr.Image(label=\"Draw Mask (white = inpaint)\", type=\"pil\")\n",
    "            inp_prompt = gr.Textbox(label=\"ðŸ“ Prompt\", lines=2, placeholder=\"e.g. a mountain with a castle\")\n",
    "            inp_negative = gr.Textbox(label=\"ðŸš« Negative Prompt\", value=\"blurry, low quality\", lines=1)\n",
    "            inp_strength = gr.Slider(0.1, 1.0, step=0.1, value=0.8, label=\"Strength\")\n",
    "            inp_cfg = gr.Slider(1.0, 20.0, step=0.5, value=7.5, label=\"CFG Scale\")\n",
    "            inp_steps = gr.Slider(10, 100, step=5, value=30, label=\"Denoising Steps\")\n",
    "            inp_seed = gr.Number(value=-1, precision=0, label=\"Seed (-1 = random)\")\n",
    "            inp_btn = gr.Button(\"Generate\")\n",
    "            inp_output = gr.Image(label=\"ðŸ–¼ï¸ Output Image\")\n",
    "            inp_btn.click(\n",
    "                generate_inpaint,\n",
    "                inputs=[inp_image, inp_mask, inp_prompt, inp_negative, inp_strength, inp_cfg, inp_steps, inp_seed],\n",
    "                outputs=inp_output,\n",
    "            )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29457f76",
   "metadata": {},
   "source": [
    "## Understanding the Latent Visualization\n",
    "\n",
    "### What are Latents?\n",
    "- **Latents** are the compressed representation of images in a lower-dimensional space (64x64x4 instead of 512x512x3)\n",
    "- The **4 channels** represent different aspects of the image content\n",
    "- During diffusion, noise is gradually removed from these latents to form the final image\n",
    "\n",
    "### Denoising Process:\n",
    "1. **Step 0**: Pure random noise in latent space\n",
    "2. **Early Steps**: Rough shapes and structures begin to emerge\n",
    "3. **Middle Steps**: More defined features and composition\n",
    "4. **Late Steps**: Fine details and refinement\n",
    "5. **Final Step**: Clean latents that decode to the final image\n",
    "\n",
    "### Channel Interpretation:\n",
    "- Each of the 4 latent channels captures different aspects of the image\n",
    "- The exact meaning of each channel is learned during training\n",
    "- Generally, they represent different frequency components and feature maps\n",
    "\n",
    "### Visualization Benefits:\n",
    "- **Debug generation**: See where the process might be going wrong\n",
    "- **Understand timing**: Observe when key features appear\n",
    "- **Compare prompts**: See how different prompts affect the denoising trajectory\n",
    "- **Optimize parameters**: Adjust CFG scale, steps, etc. based on intermediate results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
