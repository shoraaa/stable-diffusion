{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ef6e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_8232\\765365510.py:22: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import gradio as gr\n",
    "import model_loader\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "ALLOW_CUDA = False\n",
    "ALLOW_MPS = False\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "elif (torch.has_mps or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
    "    DEVICE = \"mps\"\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3eeafa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/vocab.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load models and tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/vocab.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/merges.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/v1-5-pruned-emaonly.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m model_loader\u001b[38;5;241m.\u001b[39mpreload_models_from_standard_weights(model_file, DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\clip\\tokenization_clip.py:306\u001b[0m, in \u001b[0;36mCLIPTokenizer.__init__\u001b[1;34m(self, vocab_file, merges_file, errors, unk_token, bos_token, eos_token, pad_token, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp \u001b[38;5;241m=\u001b[39m BasicTokenizer(strip_accents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, do_split_on_punc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfix_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/vocab.json'"
     ]
    }
   ],
   "source": [
    "# Load models and tokenizer\n",
    "tokenizer = CLIPTokenizer(\"../data/vocab.json\", merges_file=\"../data/merges.txt\")\n",
    "model_file = \"../data/v1-5-pruned-emaonly.ckpt\"\n",
    "models = model_loader.preload_models_from_standard_weights(model_file, DEVICE)\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41c725",
   "metadata": {},
   "source": [
    "## üîß Core Inpainting Function\n",
    "\n",
    "The main function that powers our web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b3449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inpainting\n",
    "\n",
    "def run_inpainting(image, mask, prompt, negative_prompt, strength, cfg_scale, num_steps, seed):\n",
    "    if image is None or mask is None:\n",
    "        raise gr.Error(\"Please upload both image and mask\")\n",
    "    if not prompt.strip():\n",
    "        raise gr.Error(\"Prompt is required\")\n",
    "\n",
    "    # Resize mask to match image\n",
    "    mask = mask.convert(\"L\").resize(image.size)\n",
    "    mask_np = np.array(mask)\n",
    "    original_size = image.size  \n",
    "\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 999999)\n",
    "\n",
    "    result = inpainting.inpaint(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        mask=mask_np,\n",
    "        uncond_prompt=negative_prompt,\n",
    "        strength=strength,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=cfg_scale,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=num_steps,\n",
    "        models=models,\n",
    "        tokenizer=tokenizer,\n",
    "        seed=seed,\n",
    "        device=DEVICE,\n",
    "        idle_device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    result_image = Image.fromarray(result)\n",
    "    result_image = result_image.resize(original_size, resample=Image.LANCZOS)\n",
    "\n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6e480",
   "metadata": {},
   "source": [
    "## üöÄ Launch Web Interface\n",
    "\n",
    "Start the web server and access your inpainting studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Stable Diffusion Inpainting\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## Inpainting (Gradio 5.41.1)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(label=\"Upload Your Image\", type=\"pil\")\n",
    "\n",
    "            mask_input = gr.Image(\n",
    "                label=\"Draw Mask (white = inpaint)\",\n",
    "                type=\"pil\"\n",
    "            )\n",
    "\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(label=\"üìù Prompt\", lines=2, placeholder=\"e.g. a mountain with a castle\")\n",
    "            negative_prompt = gr.Textbox(label=\"üö´ Negative Prompt\", value=\"blurry, low quality\", lines=1)\n",
    "            strength = gr.Slider(0.1, 1.0, step=0.1, value=0.8, label=\"Strength\")\n",
    "            cfg_scale = gr.Slider(1.0, 20.0, step=0.5, value=7.5, label=\"CFG Scale\")\n",
    "            num_steps = gr.Slider(10, 100, step=5   , value=30, label=\"Denoising Steps\")\n",
    "            seed = gr.Number(value=-1, precision=0, label=\"Seed (-1 = random)\")\n",
    "            run_button = gr.Button(\"üöÄ Generate\")\n",
    "\n",
    "    output_image = gr.Image(label=\"üñºÔ∏è Output Image\")\n",
    "\n",
    "    run_button.click(\n",
    "        fn=run_inpainting,\n",
    "        inputs=[image_input, mask_input, prompt, negative_prompt, strength, cfg_scale, num_steps, seed],\n",
    "        outputs=output_image\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu)",
   "language": "python",
   "name": "torch-gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
